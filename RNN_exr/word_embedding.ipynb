{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c142cba2",
   "metadata": {},
   "source": [
    "# 단어 임베딩\n",
    "## Embedding 층을 사용해 단어 임베딩 학습하기\n",
    "* 단어 벡터 사이에 조금 더 추상적이고 기하학적인 관계를 얻기 위해 단어 사이에 있는 의미 관계를 반영하기 \n",
    "    * 언어를 기하학적 공간에 매핑하기 \n",
    "    * 잘 구축된 임베딩 공간에서는 동의어가 비슷한 단어 벡터로 임베딩될 것.. \n",
    "    * 일반적으로 두 단어 벡터 사이의 거리(L2 거리)는 이 단어 사이의 의미 거리와 관계(서로 의미가 다른 단어는 멀리 떨어진 위치에, 비슷한 단어들은 가까이)\n",
    "\n",
    "### ```Embedding```\n",
    "* 크기가 (samples, sequence_length)인 2D 정수 텐서를 입력받음\n",
    "* 크기가 (samples, sequence_length, embedding_dimensionality)인 3D 실수형 텐서를 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc81c68",
   "metadata": {},
   "source": [
    "### Embedding 층의 객체 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "285350ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "# 가능한 토큰의 개수(1,000: 단어 인덱스 최댓값 + 1), 임베딩 차원(64)\n",
    "embedding_layer = Embedding(1000, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fbfed0",
   "metadata": {},
   "source": [
    "### Embedding층에 사용할 IMDB 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b55a1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "# 특성으로 사용할 단어의 수\n",
    "max_features = 10000\n",
    "# 사용할 텍스트의 길이(가장 자주 사용되는 max_features 개의 단어만 사용)\n",
    "maxlen = 20\n",
    "\n",
    "# 정수 리스트로 데이터 로드\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# 리스트를 (samples, maxlen) 크기의 2D 정수 텐서로 변환\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210b1657",
   "metadata": {},
   "source": [
    "### IMDB 데이터에 Embedding층과 분류기 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71caed91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 20, 8)             80000     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 160)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 161       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-28 17:17:10.796218: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-03-28 17:17:10.796516: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-03-28 17:17:10.908050: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-03-28 17:17:11.116960: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "621/625 [============================>.] - ETA: 0s - loss: 0.6737 - acc: 0.6116"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-28 17:17:16.421345: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 6s 9ms/step - loss: 0.6736 - acc: 0.6119 - val_loss: 0.6287 - val_acc: 0.6932\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.5499 - acc: 0.7490 - val_loss: 0.5303 - val_acc: 0.7282\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.4653 - acc: 0.7847 - val_loss: 0.5017 - val_acc: 0.7446\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.4239 - acc: 0.8055 - val_loss: 0.4931 - val_acc: 0.7534\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.3963 - acc: 0.8231 - val_loss: 0.4925 - val_acc: 0.7556\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.3733 - acc: 0.8355 - val_loss: 0.4974 - val_acc: 0.7534\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.3535 - acc: 0.8487 - val_loss: 0.5021 - val_acc: 0.7524\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.3345 - acc: 0.8594 - val_loss: 0.5102 - val_acc: 0.7502\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.3172 - acc: 0.8678 - val_loss: 0.5179 - val_acc: 0.7494\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.3003 - acc: 0.8763 - val_loss: 0.5273 - val_acc: 0.7486\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "# 나중에 임베딩된 입력을 Flatten 층에서 펼치기 위해 Embedding 층에 input_length를 지정\n",
    "model.add(Embedding(10000, 8, input_length=maxlen))\n",
    "# Embedding 층의 출력 크기는 (samples, maxlen, 8)가 된다!\n",
    "\n",
    "# 3D 임베딩 텐서를 (samples, maxlen * 8) 크기의 2D 텐서로 펼치기\n",
    "model.add(Flatten())\n",
    "\n",
    "# 분류기 추가\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0133bafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7429000318050385\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.mean(history.history['val_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d945aee4",
   "metadata": {},
   "source": [
    "---\n",
    "## 사전 훈련된 단어 임베딩 사용하기\n",
    "\n",
    "> 풀려는 문제와 함께 단어 임베딩을 학습하는 대신에 미리 계산된 임베딩 공간에서 임베딩 벡터를 로드하기 \n",
    "* 뛰어난 구조와 유용한 성질을 가진 임베딩 공간 -> 언어 구조의 일반적인 측면을 잡아낼 수 있음! \n",
    "* 자연어 처리에서 사전 훈련된 단어 임베딩을 사용하는 이유?: 충분한 데이터가 없어서 자신만의 좋은 특성을 학습하지 못하지만 꽤 일반적인 특성이 필요할 때..\n",
    "\n",
    "### 토큰화 되지 않은 IMDB 원본 데이터 받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d3433b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "imdb_dir = '../datasets/aclImdb'\n",
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname), encoding='utf8')\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381e9249",
   "metadata": {},
   "source": [
    "### 데이터 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e57f36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88582개의 고유한 토큰을 찾았습니다.\n",
      "데이터 텐서의 크기: (25000, 100)\n",
      "레이블 텐서의 크기: (25000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 100  # 100개 단어 이후는 버리기\n",
    "training_samples = 200  # 훈련 샘플: 200개\n",
    "validation_samples = 10000  # 검증 샘플: 10,000개\n",
    "max_words = 10000  # 데이터셋에서 가장 빈도 높은 10,000개의 단어만 사용\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('%s개의 고유한 토큰을 찾았습니다.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "print('데이터 텐서의 크기:', data.shape)\n",
    "print('레이블 텐서의 크기:', labels.shape)\n",
    "\n",
    "# 데이터를 훈련 세트와 검증 세트로 분할\n",
    "# 샘플이 순서대로 있기 때문에 (부정 샘플이 모두 나온 후에 긍정 샘플 나옴) 먼저 데이터를 섞기\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7ad81b",
   "metadata": {},
   "source": [
    "### GloVe 단어 임베딩 전처리\n",
    ".txt파일을 파싱하여 단어에 상응하는 벡터 표현을 매핑하는 인덱스 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f273b9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000개의 단어 벡터를 찾았습니다.\n"
     ]
    }
   ],
   "source": [
    "glove_dir = '../datasets/glove-6b'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('%s개의 단어 벡터를 찾았습니다.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ce9995",
   "metadata": {},
   "source": [
    "```Embedding``` 층에 주입할 수 있도록 임베딩 행렬을 만들기  \n",
    "행렬의 크기: (max_words, embedding_dim)  \n",
    "행렬의 i번째 원소: (토큰화로 만든) 단어 인덱스의 i번째 단어에 상응하는 embedding_dim 차원 벡터  \n",
    "인덱스 0: 어떤 단어나 토큰도 아닐 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df589389",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if i < max_words:\n",
    "        if embedding_vector is not None:\n",
    "            # 임베딩 인덱스에 없는 단어는 모두 0으로\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb36039",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
